{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pd.read_csv(\"data_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to remove suprise and calm emotions from the dataset\n",
    "data_path = data_path[(data_path.Emotions != 'calm') & (data_path.Emotions != 'surprise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function\n",
    "def pad_features(features, max_len=100):\n",
    "    return pad_sequences(features, maxlen=max_len, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plp_features(audio_path, sr=16000, order=13):\n",
    "\n",
    "    # Pre-emphasis\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_audio = np.append(audio_path[0], audio_path[1:] - pre_emphasis * audio_path[:-1])\n",
    "\n",
    "    # Framing\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "    frame_length, frame_step = frame_size * sr, frame_stride * sr\n",
    "    signal_length = len(emphasized_audio)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step)) + 1\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_audio, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(\n",
    "        np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    \n",
    "    # Windowing\n",
    "    # frames *= np.hamming(frame_length)\n",
    "    frames *= np.hanning(frame_length)\n",
    "    # frames *= np.blackman(frame_length)\n",
    "\n",
    "    # Fourier-Transform and Power Spectrum\n",
    "    NFFT = 512\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "\n",
    "    # Filter Banks\n",
    "    nfilt = 26\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * np.log10(1 + (sr / 2) / 700))\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sr)\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])\n",
    "        f_m = int(bin[m])\n",
    "        f_m_plus = int(bin[m + 1])\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "\n",
    "    # Cepstral Coefficients\n",
    "    num_ceps = order\n",
    "    cep_lifter = 22\n",
    "    cepstral_coefficients = dct(filter_banks, type=2, axis=1, norm='ortho')[:, :num_ceps]\n",
    "\n",
    "    # Liftering\n",
    "    nframes, ncoeff = cepstral_coefficients.shape\n",
    "    n = np.arange(ncoeff)\n",
    "    lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n",
    "    cepstral_coefficients *= lift\n",
    "\n",
    "    return cepstral_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Function to pad features to a fixed length\n",
    "def pad_features(feature, max_len):\n",
    "    if feature.shape[0] < max_len:\n",
    "        # Padding along the sequence dimension (axis 0)\n",
    "        pad_width = max_len - feature.shape[0]\n",
    "        feature = np.pad(feature, ((0, pad_width), (0, 0)), mode='constant')\n",
    "    else:\n",
    "        # Trim if it's longer than max_len\n",
    "        feature = feature[:max_len, :]\n",
    "    return feature\n",
    "\n",
    "X_combined, Y = [], []\n",
    "max_len = 100  # Set the desired maximum length for each sequence\n",
    "\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "    data, sample_rate = librosa.load(path)\n",
    "    \n",
    "    # Extract MFCC and PLP features\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T\n",
    "    plp = extract_plp_features(data, sr=sample_rate)\n",
    "    \n",
    "    # Pad MFCC and PLP to the same length (max_len)\n",
    "    mfcc = pad_features(mfcc, max_len=max_len)\n",
    "    plp = pad_features(plp, max_len=max_len)\n",
    "    \n",
    "    # Concatenate MFCC and PLP features along the feature dimension (axis 1)\n",
    "    combined_features = np.concatenate((mfcc, plp), axis=1)\n",
    "\n",
    "    # Append to lists\n",
    "    X_combined.append(combined_features)\n",
    "    Y.append(emotion)\n",
    "    \n",
    "# Convert lists to NumPy arrays\n",
    "X_combined = np.array(X_combined)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Save X and Y as .npy files\n",
    "np.save('Combined_X_features.npy', X_combined)\n",
    "np.save('Combined_Y_labels.npy', Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved X and Y\n",
    "X = np.load('Combined_X_features.npy', allow_pickle=True)\n",
    "Y = np.load('Combined_Y_labels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a list that has the unique emotions\n",
    "unique_emotions = data_path.Emotions.unique()\n",
    "# unique_emotions\n",
    "# convert to list\n",
    "unique_emotions = list(unique_emotions)\n",
    "unique_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y.\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
    "encoded_labels = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense,GlobalMaxPooling1D, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_model(x_train, y_train, x_test, y_test, num_classes):\n",
    "    \n",
    "    input_shape = (100, x_train.shape[2])\n",
    "    audio_model = Sequential([\n",
    "        \n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        # Dropout(0.2),\n",
    "        \n",
    "        # Bidirectional(LSTM(64)),\n",
    "        # Dropout(0.2),\n",
    "        \n",
    "        GlobalMaxPooling1D(),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    audio_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = audio_model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.001, mode='min', restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "    ])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = audio_model.evaluate(x_test, y_test)\n",
    "    print(f\"Audio Model Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return audio_model, history\n",
    "\n",
    "# Train the audio model with pre-extracted features\n",
    "audio_model, history = build_model(x_train, y_train, x_test, y_test, num_classes=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming `Y` is the original list of labels (before one-hot encoding)\n",
    "# Fit the LabelEncoder on the original labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.argmax(y_train, axis=1))  # Fit LabelEncoder on the integer-encoded classes\n",
    "\n",
    "# Since `y_test` is one-hot encoded, convert it back to the original class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = audio_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions from one-hot encoding to class labels\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Get the unique class labels using the fitted label encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred_classes, class_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
