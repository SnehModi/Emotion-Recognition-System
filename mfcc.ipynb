{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pd.read_csv(\"data_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ravdess data\n",
    "ravdess = pd.read_csv(\"Data/Ravdess.csv\")\n",
    "ravdess = ravdess[(ravdess.Emotions != 'neutral')]\n",
    "data_path = ravdess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = pd.read_csv(\"Data/Tess.csv\")\n",
    "data_path = tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "crema_d = pd.read_csv(\"Data/Crema.csv\")\n",
    "data_path = crema_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee = pd.read_csv(\"Data/Savee.csv\")\n",
    "savee = savee[(savee.Emotions != 'neutral')]\n",
    "data_path = savee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to remove suprise and calm emotions from the dataset\n",
    "data_path = data_path[(data_path.Emotions != 'calm') & (data_path.Emotions != 'surprise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function\n",
    "def pad_features(features, max_len=100):\n",
    "    return pad_sequences(features, maxlen=max_len, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "total_duration = 0\n",
    "file_count = 0\n",
    "\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "    data, sample_rate = librosa.load(path)\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T\n",
    "    X.append(mfcc)       \n",
    "    Y.append(emotion)\n",
    "    \n",
    "X = pad_features(X, max_len=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ravdess = 3.71\n",
    "TESS = 2.05\n",
    "CremaD = 2.54\n",
    "Savee = 3.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and Y to numpy arrays (if not already)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert X and Y to numpy arrays (if not already)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Save X and Y as .npy files\n",
    "np.save('X_features.npy', X)\n",
    "np.save('Y_labels.npy', Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_mfcc.npy', X)\n",
    "np.save('Y_mfcc_labels.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(Y), data_path.Path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved X and Y\n",
    "X = np.load('X_mfcc.npy', allow_pickle=True)\n",
    "Y = np.load('Y_mfcc_labels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a list that has the unique emotions\n",
    "unique_emotions = data_path.Emotions.unique()\n",
    "# unique_emotions\n",
    "# convert to list\n",
    "unique_emotions = list(unique_emotions)\n",
    "unique_emotions\n",
    "# count the number of unique emotions\n",
    "n_classes = len(unique_emotions)\n",
    "n_classes, unique_emotions\n",
    "emotions = np.array(unique_emotions)\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y.\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
    "encoded_labels = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense,GlobalMaxPooling1D, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_model(x_train, y_train, x_test, y_test, num_classes):\n",
    "    \n",
    "    input_shape = (100, x_train.shape[2])\n",
    "    audio_model = Sequential([\n",
    "        # Convolutional layer\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "               kernel_regularizer=l2(0.01), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', \n",
    "               kernel_regularizer=l2(0.01), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    audio_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = audio_model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, min_delta=0.001, mode='min', restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath='best_mfcc_model.keras', monitor='val_loss', save_best_only=True)\n",
    "    ])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = audio_model.evaluate(x_test, y_test)\n",
    "    print(f\"Audio Model Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return audio_model, history\n",
    "\n",
    "# Train the audio model with pre-extracted features\n",
    "audio_model, history = build_model(x_train, y_train, x_test, y_test, num_classes=n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_emotion_classification(model, x_test, y_test, class_names):\n",
    "    # Get predictions\n",
    "    predictions = model.predict(x_test)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # 1. Class-wise Performance Metrics\n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_names))\n",
    "    \n",
    "    # 2. Confusion Matrix with custom formatting\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Per-class Performance Analysis\n",
    "    class_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-Score': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        true_class = (true_classes == i)\n",
    "        pred_class = (predicted_classes == i)\n",
    "        \n",
    "        tp = np.sum(true_class & pred_class)\n",
    "        fp = np.sum(~true_class & pred_class)\n",
    "        fn = np.sum(true_class & ~pred_class)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        class_metrics['Precision'].append(precision)\n",
    "        class_metrics['Recall'].append(recall)\n",
    "        class_metrics['F1-Score'].append(f1)\n",
    "    \n",
    "    # Plot per-class metrics\n",
    "    metrics_df = pd.DataFrame(class_metrics, index=class_names)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics_df.plot(kind='bar', width=0.8)\n",
    "    plt.title('Per-class Performance Metrics')\n",
    "    plt.xlabel('Emotion')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Emotion Correlation Analysis\n",
    "    emotion_probs = pd.DataFrame(predictions, columns=class_names)\n",
    "    correlation_matrix = emotion_probs.corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Emotion Prediction Correlation Matrix')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df, correlation_matrix\n",
    "\n",
    "# Usage\n",
    "# Assuming you have your class names in a list\n",
    "class_names = unique_emotions  # replace with your actual emotion classes\n",
    "metrics_df, correlation_matrix = analyze_emotion_classification(audio_model, x_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import chi2_contingency\n",
    "import networkx as nx\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "class EmotionRelationshipAnalysis:\n",
    "    def __init__(self, model, x_test, y_test, class_names):\n",
    "        self.model = model\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.class_names = class_names\n",
    "        self.predictions = model.predict(x_test)\n",
    "        self.true_labels = np.argmax(y_test, axis=1)\n",
    "        self.pred_labels = np.argmax(self.predictions, axis=1)\n",
    "        \n",
    "    def analyze_confusion_patterns(self):\n",
    "        \"\"\"Analyze and visualize confusion patterns between emotions\"\"\"\n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(self.true_labels, self.pred_labels)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title('Normalized Confusion Matrix: Emotion Misclassification Patterns')\n",
    "        plt.ylabel('True Emotion')\n",
    "        plt.xlabel('Predicted Emotion')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze most common confusions\n",
    "        confusion_pairs = []\n",
    "        for i in range(len(self.class_names)):\n",
    "            for j in range(len(self.class_names)):\n",
    "                if i != j:\n",
    "                    confusion_pairs.append({\n",
    "                        'True': self.class_names[i],\n",
    "                        'Predicted': self.class_names[j],\n",
    "                        'Rate': cm_normalized[i, j]\n",
    "                    })\n",
    "        \n",
    "        confusion_df = pd.DataFrame(confusion_pairs)\n",
    "        print(\"\\nTop Confusion Pairs:\")\n",
    "        print(confusion_df.sort_values('Rate', ascending=False).head())\n",
    "        \n",
    "        return cm_normalized, confusion_df\n",
    "    \n",
    "    def emotion_correlation_network(self):\n",
    "        \"\"\"Create and visualize emotion correlation network\"\"\"\n",
    "        # Calculate correlation between prediction probabilities\n",
    "        emotion_probs = pd.DataFrame(self.predictions, columns=self.class_names)\n",
    "        correlations = emotion_probs.corr()\n",
    "        \n",
    "        # Create network graph\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes and edges\n",
    "        for i in range(len(self.class_names)):\n",
    "            G.add_node(self.class_names[i])\n",
    "            for j in range(i+1, len(self.class_names)):\n",
    "                if abs(correlations.iloc[i, j]) > 0.1:  # Threshold for visualization\n",
    "                    G.add_edge(self.class_names[i], self.class_names[j],\n",
    "                             weight=abs(correlations.iloc[i, j]))\n",
    "        \n",
    "        # Draw network\n",
    "        pos = nx.spring_layout(G)\n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] * 2 for u, v in edges]\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                             node_size=1000, alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5)\n",
    "        nx.draw_networkx_labels(G, pos)\n",
    "        \n",
    "        plt.title(\"Emotion Relationship Network\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return correlations\n",
    "    \n",
    "    def emotion_clustering(self):\n",
    "        \"\"\"Perform hierarchical clustering of emotions\"\"\"\n",
    "        # Get feature representations for each emotion\n",
    "        emotion_features = []\n",
    "        for i in range(len(self.class_names)):\n",
    "            mask = (self.true_labels == i)\n",
    "            emotion_features.append(np.mean(self.x_test[mask], axis=0))\n",
    "        \n",
    "        # Perform hierarchical clustering\n",
    "        linkage_matrix = linkage(emotion_features, 'ward')\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(linkage_matrix, labels=self.class_names)\n",
    "        plt.title('Hierarchical Clustering of Emotions')\n",
    "        plt.xlabel('Emotions')\n",
    "        plt.ylabel('Distance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return linkage_matrix\n",
    "    \n",
    "    def analyze_feature_importance_per_emotion(self):\n",
    "        \"\"\"Analyze which features are most important for each emotion\"\"\"\n",
    "        feature_importance = {}\n",
    "        \n",
    "        for i, emotion in enumerate(self.class_names):\n",
    "            # Get samples for this emotion\n",
    "            mask = (self.true_labels == i)\n",
    "            emotion_samples = self.x_test[mask]\n",
    "            \n",
    "            # Calculate mean feature values\n",
    "            mean_features = np.mean(emotion_samples, axis=0)\n",
    "            std_features = np.std(emotion_samples, axis=0)\n",
    "            \n",
    "            # Store importance scores\n",
    "            feature_importance[emotion] = {\n",
    "                'mean': mean_features,\n",
    "                'std': std_features\n",
    "            }\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        for emotion in self.class_names:\n",
    "            plt.plot(feature_importance[emotion]['mean'], label=emotion)\n",
    "        \n",
    "        plt.title('Feature Patterns Across Emotions')\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Mean Feature Value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return feature_importance\n",
    "    \n",
    "    def transition_analysis(self):\n",
    "        \"\"\"Analyze temporal transitions between emotions\"\"\"\n",
    "        # Create transition matrix\n",
    "        n_emotions = len(self.class_names)\n",
    "        transitions = np.zeros((n_emotions, n_emotions))\n",
    "        \n",
    "        for i in range(len(self.pred_labels)-1):\n",
    "            transitions[self.pred_labels[i], self.pred_labels[i+1]] += 1\n",
    "        \n",
    "        # Normalize\n",
    "        row_sums = transitions.sum(axis=1)\n",
    "        transitions_normalized = transitions / row_sums[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(transitions_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                    xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title('Emotion Transition Probabilities')\n",
    "        plt.ylabel('Current Emotion')\n",
    "        plt.xlabel('Next Emotion')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return transitions_normalized\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = EmotionRelationshipAnalysis(audio_model, x_test, y_test, class_names)\n",
    "\n",
    "# 1. Analyze confusion patterns\n",
    "cm_normalized, confusion_df = analyzer.analyze_confusion_patterns()\n",
    "\n",
    "# 2. Create emotion correlation network\n",
    "correlations = analyzer.emotion_correlation_network()\n",
    "\n",
    "# 3. Perform emotion clustering\n",
    "# linkage_matrix = analyzer.emotion_clustering()\n",
    "\n",
    "# 4. Analyze feature importance per emotion\n",
    "feature_importance = analyzer.analyze_feature_importance_per_emotion()\n",
    "\n",
    "# 5. Analyze emotion transitions\n",
    "transitions = analyzer.transition_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming `Y` is the original list of labels (before one-hot encoding)\n",
    "# Fit the LabelEncoder on the original labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.argmax(y_train, axis=1))  # Fit LabelEncoder on the integer-encoded classes\n",
    "\n",
    "# Since `y_test` is one-hot encoded, convert it back to the original class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = audio_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions from one-hot encoding to class labels\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Get the unique class labels using the fitted label encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred_classes, class_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the training history\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize UMAP for 2D\n",
    "# umap_2d = umap.UMAP(n_components=2, random_state=42)\n",
    "\n",
    "# flat_features = X.reshape(X.shape[0], -1)\n",
    "\n",
    "# # Fit and transform the flattened features\n",
    "# umap_features_2d = umap_2d.fit_transform(flat_features)\n",
    "\n",
    "# # Create the scatter plot\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.scatterplot(x=umap_features_2d[:, 0], y=umap_features_2d[:, 1], hue=encoded_labels, palette='viridis', s=60)\n",
    "\n",
    "# # Add labels and title\n",
    "# plt.title('UMAP Visualization of Features Based on Emotions')\n",
    "# plt.xlabel('UMAP Feature 1')\n",
    "# plt.ylabel('UMAP Feature 2')\n",
    "# plt.legend(title='Emotion', labels=unique_emotions)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Emotions' column and pick the first sample for each emotion\n",
    "samples = data_path.groupby('Emotions').first().reset_index()\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['Emotions'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "audio_file = samples['Path'][0]\n",
    "y, sr = librosa.load(audio_file, sr=16000)\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform of the Audio Sample')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('MFCC Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = 'your_audio_sample.wav'\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Remove silence at the start and end\n",
    "y, _ = librosa.effects.trim(y)\n",
    "\n",
    "# Create time axis for the waveform and spectrogram\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "time = np.linspace(0., duration, len(y))\n",
    "\n",
    "# Create the figure and axis for both plots\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Plot the waveform\n",
    "ax[0].plot(time, y)\n",
    "ax[0].set(title='Waveform', xlabel='Time (s)', ylabel='Amplitude')\n",
    "\n",
    "# Plot the spectrogram\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', ax=ax[1])\n",
    "ax[1].set(title='Spectrogram', xlabel='Time (s)', ylabel='Frequency (Hz)')\n",
    "ax[1].colorbar(format='%+2.0f dB')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = samples['Path'][0]  # Replace with your audio file path\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Remove silences at the start and end\n",
    "y_trimmed, _ = librosa.effects.trim(y)\n",
    "\n",
    "# Create the figure with 3 subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# Plot the waveform\n",
    "axs[0].set_title('Waveform (Trimmed)')\n",
    "librosa.display.waveshow(y_trimmed, sr=sr, ax=axs[0])\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "axs[1].set_title('Mel Spectrogram')\n",
    "S = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img1 = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000, ax=axs[1], cmap='magma')\n",
    "axs[1].set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(img1, ax=axs[1], format=\"%+2.0f dB\")\n",
    "\n",
    "# Plot the MFCC spectrogram\n",
    "axs[2].set_title('MFCC Spectrogram')\n",
    "MFCCs = librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=13)\n",
    "img2 = librosa.display.specshow(MFCCs, x_axis='time', sr=sr, ax=axs[2], cmap='viridis')\n",
    "axs[2].set_ylabel('MFCC Coefficients')\n",
    "fig.colorbar(img2, ax=axs[2], format=\"%+2.0f dB\")\n",
    "\n",
    "# Add a shared X-axis label\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = samples['Path'][0]  # Replace with your audio file path\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Remove silences at the start and end\n",
    "y_trimmed, _ = librosa.effects.trim(y)\n",
    "\n",
    "# Calculate time array for waveform\n",
    "time = np.linspace(0, len(y_trimmed) / sr, len(y_trimmed))\n",
    "\n",
    "# Create Mel spectrogram\n",
    "S = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=128, fmax=8000)\n",
    "\n",
    "# Create MFCC spectrogram\n",
    "MFCCs = librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=13)\n",
    "\n",
    "# Create spectrogram using STFT\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y_trimmed)), ref=np.max)\n",
    "\n",
    "# Create the figure with 4 subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 16), sharex=True)\n",
    "\n",
    "# Plot the waveform\n",
    "axs[0].set_title('Waveform')\n",
    "axs[0].plot(time, y_trimmed, color='blue')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot the spectrogram\n",
    "axs[1].set_title('Spectrogram (STFT)')\n",
    "img1 = axs[1].imshow(\n",
    "    D, \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 0, sr // 2], \n",
    "    cmap='viridis'\n",
    ")\n",
    "axs[1].set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(img1, ax=axs[1], format=\"%+2.0f dB\")\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "axs[2].set_title('Mel Spectrogram')\n",
    "mel_time = np.linspace(0, len(y_trimmed) / sr, S.shape[1])  # Align Mel spectrogram timeline\n",
    "img2 = axs[2].imshow(\n",
    "    librosa.power_to_db(S, ref=np.max), \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 0, 8000], \n",
    "    cmap='magma'\n",
    ")\n",
    "axs[2].set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(img2, ax=axs[2], format=\"%+2.0f dB\")\n",
    "\n",
    "# Plot the MFCC spectrogram\n",
    "axs[3].set_title('MFCC Spectrogram')\n",
    "mfcc_time = np.linspace(0, len(y_trimmed) / sr, MFCCs.shape[1])  # Align MFCC spectrogram timeline\n",
    "img3 = axs[3].imshow(\n",
    "    MFCCs, \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 1, 13], \n",
    "    cmap='plasma'\n",
    ")\n",
    "axs[3].set_ylabel('MFCC Coefficients')\n",
    "fig.colorbar(img3, ax=axs[3], format=\"%+2.0f dB\")\n",
    "\n",
    "# Add a shared X-axis label\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = samples['Path'][0]  # Replace with your audio file path\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Remove silences at the start and end\n",
    "y_trimmed, _ = librosa.effects.trim(y)\n",
    "\n",
    "# Calculate time array for waveform\n",
    "time = np.linspace(0, len(y_trimmed) / sr, len(y_trimmed))\n",
    "\n",
    "# Create Mel spectrogram\n",
    "S = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=128, fmax=8000)\n",
    "\n",
    "# Create MFCC spectrogram\n",
    "MFCCs = librosa.feature.mfcc(y=y_trimmed, sr=sr, n_mfcc=13)\n",
    "\n",
    "# Create the figure with 3 subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# Plot the waveform\n",
    "axs[0].set_title('Waveform (Trimmed)')\n",
    "axs[0].plot(time, y_trimmed, color='blue')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot the Mel spectrogram (raw values, no dB conversion)\n",
    "axs[1].set_title('Mel Spectrogram (Raw)')\n",
    "mel_time = np.linspace(0, len(y_trimmed) / sr, S.shape[1])  # Align Mel spectrogram timeline\n",
    "img1 = axs[1].imshow(\n",
    "    S, \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 0, 8000], \n",
    "    cmap='magma'\n",
    ")\n",
    "axs[1].set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(img1, ax=axs[1], format=\"%+2.0f\")\n",
    "\n",
    "# Plot the MFCC spectrogram (raw values)\n",
    "axs[2].set_title('MFCC Spectrogram (Raw)')\n",
    "mfcc_time = np.linspace(0, len(y_trimmed) / sr, MFCCs.shape[1])  # Align MFCC spectrogram timeline\n",
    "img2 = axs[2].imshow(\n",
    "    MFCCs, \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 1, 13], \n",
    "    cmap='viridis'\n",
    ")\n",
    "axs[2].set_ylabel('MFCC Coefficients')\n",
    "fig.colorbar(img2, ax=axs[2], format=\"%+2.0f\")\n",
    "\n",
    "# Add a shared X-axis label\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = samples['Path'][0]  # Replace with your audio file path\n",
    "y, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "# Remove silences\n",
    "y_trimmed, _ = librosa.effects.trim(y, top_db=20)  # Adjust top_db for sensitivity\n",
    "\n",
    "# Create the Mel Spectrogram\n",
    "S = librosa.feature.melspectrogram(y_trimmed, sr=sr, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "# Plot the waveform and Mel spectrogram\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 6), gridspec_kw={'height_ratios': [1, 2]}, sharex=True)\n",
    "\n",
    "# Plot the waveform\n",
    "librosa.display.waveshow(y_trimmed, sr=sr, ax=axs[0], color='blue')\n",
    "axs[0].set_title('Trimmed Waveform', fontsize=14)\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].grid()\n",
    "\n",
    "# Plot the Mel Spectrogram\n",
    "img = librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', fmax=8000, ax=axs[1], cmap='viridis')\n",
    "axs[1].set_title('Mel Spectrogram', fontsize=14)\n",
    "axs[1].set_ylabel('Frequency (Hz)')\n",
    "axs[1].grid()\n",
    "fig.colorbar(img, ax=axs[1], format='%+2.0f dB')\n",
    "\n",
    "# Set shared x-axis label\n",
    "axs[1].set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "mfcc_df = pd.DataFrame(mfccs.T, columns=[f'MFCC {i+1}' for i in range(mfccs.shape[0])])\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=mfcc_df)\n",
    "plt.title('Boxplot of MFCC Coefficients')\n",
    "plt.xlabel('MFCC Coefficient Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_mfcc.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_features = X\n",
    "labels = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute mean across time steps for each sample\n",
    "mean_mfcc = np.mean(mfcc_features, axis=1)  # Shape: (n_samples, n_coefficients)\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "df = pd.DataFrame(mean_mfcc, columns=[f\"MFCC {i+1}\" for i in range(mean_mfcc.shape[1])])\n",
    "df['Emotion'] = labels\n",
    "\n",
    "# Melt for grouped bar plot\n",
    "df_melted = df.melt(id_vars='Emotion', var_name='MFCC Coefficient', value_name='Mean Value')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_melted, x='MFCC Coefficient', y='Mean Value', hue='Emotion', ci=\"sd\")\n",
    "plt.title('Mean MFCC Coefficients Across Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the desired MFCC coefficients\n",
    "selected_mfcc = [\"MFCC 1\", \"MFCC 2\", \"MFCC 4\", \"MFCC 6\"]\n",
    "df_selected = df.melt(id_vars='Emotion', var_name='MFCC Coefficient', value_name='Mean Value')\n",
    "df_selected = df_selected[df_selected['MFCC Coefficient'].isin(selected_mfcc)]\n",
    "\n",
    "# Plot the selected coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_selected, x='MFCC Coefficient', y='Mean Value', hue='Emotion', ci=\"sd\")\n",
    "plt.title('Mean of Selected MFCC Coefficients Across Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Flatten MFCC features (mean across time steps)\n",
    "mfcc_flattened = np.mean(mfcc_features, axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "mfcc_scaled = scaler.fit_transform(mfcc_flattened)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "mfcc_tsne = tsne.fit_transform(mfcc_scaled)\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(8, 7))\n",
    "for emotion in emotions:\n",
    "    idx = np.where(labels == emotion)\n",
    "    plt.scatter(mfcc_tsne[idx, 0], mfcc_tsne[idx, 1], label=emotion, alpha=0.7)\n",
    "plt.title('t-SNE Visualization of MFCC Features')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_mfcc_heatmap(mfcc_features, emotion_label, title=\"MFCC Heatmap\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mfcc_features.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar(label=\"Magnitude\")\n",
    "    plt.title(f\"{title}: {emotion_label}\")\n",
    "    plt.xlabel(\"Time Frames\")\n",
    "    plt.ylabel(\"MFCC Coefficients\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "emotion_label = \"Happy\"\n",
    "mfcc_features = np.random.rand(140, 13)  # Replace with real MFCC data\n",
    "plot_mfcc_heatmap(mfcc_features, emotion_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each audio file and extract MFCCs\n",
    "mfcc_features = {}\n",
    "for emotion, file_path in zip(samples[\"Emotions\"], samples[\"Path\"]):\n",
    "    y, sr = librosa.load(file_path, sr=16000)  # Load the audio with 16kHz sampling rate\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Extract 13 MFCC coefficients\n",
    "    mfcc_features[emotion] = mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MFCC heatmaps\n",
    "plt.figure(figsize=(15, 5))  # Set figure size for better visuals\n",
    "\n",
    "for i, (emotion, mfcc) in enumerate(mfcc_features.items()):\n",
    "    plt.subplot(1, len(mfcc_features), i + 1)  # Create subplots dynamically\n",
    "    librosa.display.specshow(mfcc, x_axis='time', sr=16000, cmap='viridis')  # Heatmap\n",
    "    plt.colorbar(label='MFCC Coeff Magnitude')\n",
    "    plt.title(f'MFCC Heatmap - {emotion.capitalize()}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Annotate the \"happy\" emotion's heatmap\n",
    "emotion = \"sad\"\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfcc_features[emotion], x_axis='time', sr=16000, cmap='viridis')\n",
    "plt.colorbar(label='MFCC Coeff Magnitude')\n",
    "plt.title(f'MFCC Heatmap - {emotion.capitalize()}')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "\n",
    "# Add a rectangle around a region of interest\n",
    "plt.gca().add_patch(plt.Rectangle((0.5, 0), 0.5, 3, fill=False, edgecolor='red', linewidth=2, linestyle='--'))\n",
    "plt.text(0.6, 1, 'High energy region', color='red', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map emotion names to numeric labels\n",
    "emotion_to_label = {emotion: idx for idx, emotion in enumerate(data_path[\"Emotions\"].unique())}\n",
    "label_to_emotion = {idx: emotion for emotion, idx in emotion_to_label.items()}\n",
    "\n",
    "# Containers for features and labels\n",
    "mfcc_features = []\n",
    "emotion_labels = []\n",
    "\n",
    "# Extract MFCC features for each sample\n",
    "for _, row in data_path.iterrows():\n",
    "    emotion = row['Emotions']\n",
    "    file_path = row['Path']\n",
    "    \n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(file_path, sr=16000)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)  # Aggregate across time frames\n",
    "    \n",
    "    # Append features and labels\n",
    "    mfcc_features.append(mfcc_mean)\n",
    "    emotion_labels.append(emotion_to_label[emotion])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "mfcc_features = np.array(mfcc_features)\n",
    "emotion_labels = np.array(emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(mfcc_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_emotion_clusters_pca(reduced_features, emotion_labels, label_to_emotion, title=\"Emotion Clusters (PCA)\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in np.unique(emotion_labels):\n",
    "        indices = emotion_labels == label\n",
    "        plt.scatter(\n",
    "            reduced_features[indices, 0],  # First principal component\n",
    "            reduced_features[indices, 1],  # Second principal component\n",
    "            label=label_to_emotion[label],  # Emotion name\n",
    "            alpha=0.7\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_emotion_clusters_pca(reduced_features, emotion_labels, label_to_emotion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = data_path\n",
    "\n",
    "# Define a container to store MFCCs grouped by emotion\n",
    "emotion_mfcc = {emotion: [] for emotion in data[\"Emotions\"].unique()}\n",
    "\n",
    "# Extract MFCC features for each file\n",
    "for _, row in data.iterrows():\n",
    "    emotion = row[\"Emotions\"]\n",
    "    file_path = row[\"Path\"]\n",
    "    \n",
    "    # Load audio file\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)  # Aggregate across time frames\n",
    "        \n",
    "        # Store the mean MFCCs for this file\n",
    "        emotion_mfcc[emotion].append(mfcc_mean)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Compute mean MFCCs for each emotion\n",
    "mean_mfcc_profiles = {emotion: np.mean(features, axis=0) for emotion, features in emotion_mfcc.items()}\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "mean_mfcc_df = pd.DataFrame(mean_mfcc_profiles)\n",
    "\n",
    "# Plot mean MFCC profiles\n",
    "plt.figure(figsize=(10, 6))\n",
    "for emotion in mean_mfcc_df.columns:\n",
    "    plt.plot(mean_mfcc_df.index, mean_mfcc_df[emotion], label=emotion)\n",
    "    \n",
    "plt.title(\"Mean MFCC Profiles Across Emotions\")\n",
    "plt.xlabel(\"MFCC Coefficients\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.legend(title=\"Emotions\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = data_path\n",
    "\n",
    "# Container for storing time-aligned MFCCs grouped by emotion\n",
    "emotion_time_mfcc = {emotion: [] for emotion in data[\"Emotions\"].unique()}\n",
    "\n",
    "# Define a fixed number of time frames for alignment\n",
    "fixed_frames = 100\n",
    "\n",
    "# Extract MFCC features and align them\n",
    "for _, row in data.iterrows():\n",
    "    emotion = row[\"Emotions\"]\n",
    "    file_path = row[\"Path\"]\n",
    "    \n",
    "    # Load audio file\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # Align MFCCs to the fixed number of frames\n",
    "        mfcc_resized = librosa.util.fix_length(mfcc, size=fixed_frames, axis=1)\n",
    "        \n",
    "        # Store the time-aligned MFCCs\n",
    "        emotion_time_mfcc[emotion].append(mfcc_resized)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Compute mean MFCCs over time for each emotion\n",
    "mean_time_mfcc = {\n",
    "    emotion: np.mean(np.array(features), axis=0) for emotion, features in emotion_time_mfcc.items()\n",
    "}\n",
    "\n",
    "# Plot MFCC dynamics over time for each emotion\n",
    "plt.figure(figsize=(12, 8))\n",
    "for emotion, mfcc_mean in mean_time_mfcc.items():\n",
    "    plt.plot(\n",
    "        np.arange(fixed_frames), \n",
    "        np.mean(mfcc_mean, axis=0),  # Mean across coefficients\n",
    "        label=emotion\n",
    "    )\n",
    "    \n",
    "plt.title(\"MFCC Dynamics Over Time for Emotions\")\n",
    "plt.xlabel(\"Time Frames\")\n",
    "plt.ylabel(\"Mean MFCC Value\")\n",
    "plt.legend(title=\"Emotions\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "data = data_path\n",
    "# Initialize a dictionary to store MFCC coefficients for each emotion\n",
    "emotion_mfcc_stats = {emotion: [] for emotion in data[\"Emotions\"].unique()}\n",
    "\n",
    "# Loop through the dataset to extract MFCC features for each file\n",
    "for _, row in data.iterrows():\n",
    "    emotion = row[\"Emotions\"]\n",
    "    file_path = row[\"Path\"]\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # Compute the mean MFCC coefficients (reduce time dimension)\n",
    "        mfcc_mean = mfcc.mean(axis=1)\n",
    "        \n",
    "        # Append the MFCC mean to the respective emotion's list\n",
    "        emotion_mfcc_stats[emotion].append(mfcc_mean)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ANOVA test results\n",
    "anova_results = []\n",
    "coeff_names = [f\"MFCC {i+1}\" for i in range(13)]\n",
    "emotion_labels = list(emotion_mfcc_stats.keys())\n",
    "\n",
    "# Loop through each MFCC coefficient to test for differences between emotions\n",
    "for coeff_idx in range(13):\n",
    "    # Collect data for the current MFCC coefficient across all emotions\n",
    "    samples = [np.array([sample[coeff_idx] for sample in emotion_mfcc_stats[emotion]])\n",
    "               for emotion in emotion_labels]\n",
    "    \n",
    "    # Perform ANOVA test\n",
    "    f_stat, p_value = f_oneway(*samples)\n",
    "    anova_results.append({\"Coefficient\": coeff_names[coeff_idx], \"F-Stat\": f_stat, \"P-Value\": p_value})\n",
    "\n",
    "# Convert ANOVA results into a DataFrame for easier visualization\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "anova_df[\"Significant\"] = anova_df[\"P-Value\"] < 0.05\n",
    "print(anova_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean MFCC coefficients for each emotion\n",
    "emotion_mfcc_means = {\n",
    "    emotion: np.mean(np.array(mfccs), axis=0) for emotion, mfccs in emotion_mfcc_stats.items()\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame for plotting\n",
    "mean_mfcc_df = pd.DataFrame(emotion_mfcc_means, index=coeff_names)\n",
    "\n",
    "# Plot the mean MFCC coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "mean_mfcc_df.plot(kind=\"bar\", figsize=(14, 8))\n",
    "plt.title(\"Mean MFCC Coefficients Across Emotions\")\n",
    "plt.xlabel(\"MFCC Coefficients\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.legend(title=\"Emotion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight significant MFCC coefficients in the plot\n",
    "significant_coeffs = anova_df[anova_df[\"Significant\"]][\"Coefficient\"].tolist()\n",
    "print(f\"Significant Coefficients: {significant_coeffs}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=mean_mfcc_df.T, ci=\"sd\", palette=\"muted\")\n",
    "plt.title(\"Mean MFCC Coefficients with Statistical Significance\")\n",
    "plt.xlabel(\"Emotion\")\n",
    "plt.ylabel(\"MFCC Mean Value\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", title=\"MFCC Coefficients\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path.Path[0]\n",
    "data_path.Emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = data_path.Path[0]  # Replace with your file path\n",
    "y, sr = librosa.load(audio_path, sr=16000)  # `sr=None` keeps the original sampling rate\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(y, sr=sr, alpha=0.5)\n",
    "plt.title(\"Waveform\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate and plot the spectrogram\n",
    "D = librosa.stft(y)  # Short-time Fourier Transform\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(S_db, sr=sr, x_axis=\"time\", y_axis=\"hz\", cmap=\"magma\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_path = \"AudioData\\sch.wav\"  # Replace with your file path\n",
    "y, sr = librosa.load(audio_path, sr=16000)  # `sr` is the sampling rate\n",
    "\n",
    "# Specify the duration (in seconds) and start time\n",
    "start_time = 5  # Start at 10 seconds (adjust as needed)\n",
    "duration = 5  # Duration of the segment in seconds\n",
    "\n",
    "# Extract the desired segment\n",
    "start_sample = int(start_time * sr)\n",
    "end_sample = int((start_time + duration) * sr)\n",
    "y_segment = y[start_sample:end_sample]\n",
    "\n",
    "# Plot the waveform of the segment\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(y_segment, sr=sr, alpha=0.5)\n",
    "plt.title(f\"Waveform (Segment {start_time}-{start_time + duration}s)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate and plot the spectrogram of the segment\n",
    "D = librosa.stft(y_segment)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(S_db, sr=sr, x_axis=\"time\", y_axis=\"hz\", cmap=\"magma\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(f\"Spectrogram (Segment {start_time}-{start_time + duration}s)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
