{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from scipy.fftpack import dct\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization, LSTM, Bidirectional\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plp_features(audio_path, sr=16000, order=13):\n",
    "\n",
    "    # Pre-emphasis\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_audio = np.append(audio_path[0], audio_path[1:] - pre_emphasis * audio_path[:-1])\n",
    "\n",
    "    # Framing\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "    frame_length, frame_step = frame_size * sr, frame_stride * sr\n",
    "    signal_length = len(emphasized_audio)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step)) + 1\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_audio, z)\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(\n",
    "        np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    \n",
    "    # Windowing\n",
    "    # frames *= np.hamming(frame_length)\n",
    "    frames *= np.hanning(frame_length)\n",
    "    # frames *= np.blackman(frame_length)\n",
    "\n",
    "    # Fourier-Transform and Power Spectrum\n",
    "    NFFT = 512\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "\n",
    "    # Filter Banks\n",
    "    nfilt = 26\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = (2595 * np.log10(1 + (sr / 2) / 700))\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sr)\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])\n",
    "        f_m = int(bin[m])\n",
    "        f_m_plus = int(bin[m + 1])\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "\n",
    "    # Cepstral Coefficients\n",
    "    num_ceps = order\n",
    "    cep_lifter = 22\n",
    "    cepstral_coefficients = dct(filter_banks, type=2, axis=1, norm='ortho')[:, :num_ceps]\n",
    "\n",
    "    # Liftering\n",
    "    nframes, ncoeff = cepstral_coefficients.shape\n",
    "    n = np.arange(ncoeff)\n",
    "    lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n",
    "    cepstral_coefficients *= lift\n",
    "\n",
    "    return cepstral_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pd.read_csv(\"data_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to remove suprise and calm emotions from the dataset\n",
    "data_path = data_path[(data_path.Emotions != 'calm') & (data_path.Emotions != 'surprise')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(data_path.Emotions)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function\n",
    "def pad_features(features, max_len=120):\n",
    "    return pad_sequences(features, maxlen=max_len, padding='post', dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "    data, sample_rate = librosa.load(path)\n",
    "    # plp = librosa.feature.plp(y=data, sr=sample_rate, n_plp=13).T\n",
    "    plp = extract_plp_features(data, sr=sample_rate)\n",
    "    X.append(plp)       \n",
    "    Y.append(emotion)\n",
    "    \n",
    "X = pad_features(X, max_len=100)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_plp.npy', X)\n",
    "np.save('Y_plp_labels.npy', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved X and Y\n",
    "X = np.load('X_plp.npy', allow_pickle=True)\n",
    "Y = np.load('Y_plp_labels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want a list that has the unique emotions\n",
    "unique_emotions = data_path.Emotions.unique()\n",
    "# unique_emotions\n",
    "# convert to list\n",
    "unique_emotions = list(unique_emotions)\n",
    "unique_emotions\n",
    "# count the number of unique emotions\n",
    "n_classes = len(unique_emotions)\n",
    "n_classes, unique_emotions\n",
    "emotions = np.array(unique_emotions)\n",
    "unique_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As this is a multiclass classification problem onehotencoding our Y.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# # Train the audio model\n",
    "# def train_audio_model(X_train, y_train, X_test, y_test, num_classes=8):\n",
    "    \n",
    "#     # Create the model\n",
    "#     input_shape = (120, X_train.shape[2])  \n",
    "#     audio_model = Sequential([\n",
    "#         Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.4),  # Increased dropout\n",
    "        \n",
    "#         # Bidirectional(LSTM(128, return_sequences=True)),\n",
    "#         # Bidirectional(LSTM(64)),\n",
    "        \n",
    "#         LSTM(128, return_sequences=True),\n",
    "#         LSTM(64),\n",
    "        \n",
    "#         Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "#         Dropout(0.5),\n",
    "#         Dense(64, activation='relu', kernel_regularizer=l2(0.01)),  # Added L2 regularization\n",
    "#         Dropout(0.4),  # Increased dropout\n",
    "#         Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model with Adam optimizer and learning rate scheduler\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#     audio_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#     # Callbacks: Early stopping, learning rate scheduler, and model checkpoint\n",
    "#     callbacks = [\n",
    "#         tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.001, mode='min', restore_best_weights=True, verbose=1),\n",
    "#         tf.keras.callbacks.ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True),\n",
    "#         tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)  # Learning rate scheduler\n",
    "#     ]\n",
    "\n",
    "#     # Train the model (optional class weights can be used if dataset is imbalanced)\n",
    "#     # class_weight = {0: 1.0, 1: 2.0, 2: 1.5, ...}  # Adjust class weights based on class distribution\n",
    "#     history = audio_model.fit(\n",
    "#         X_train, y_train, \n",
    "#         epochs=100, \n",
    "#         batch_size=32, \n",
    "#         validation_data=(X_test, y_test), \n",
    "#         callbacks=callbacks\n",
    "#         # ,class_weight=class_weight  # Uncomment if class weights are to be used\n",
    "#     )\n",
    "\n",
    "#     # Evaluate the model on the test set\n",
    "#     loss, accuracy = audio_model.evaluate(X_test, y_test)\n",
    "#     print(f\"Audio Model Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "#     return audio_model, history\n",
    "\n",
    "# # Train the audio model with pre-extracted features\n",
    "# audio_model, history = train_audio_model(x_train, y_train, x_test, y_test, num_classes=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense,GlobalMaxPooling1D, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_model(x_train, y_train, x_test, y_test, num_classes):\n",
    "    \n",
    "    input_shape = (100, x_train.shape[2])\n",
    "    audio_model = Sequential([\n",
    "        # Convolutional layer\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', \n",
    "               kernel_regularizer=l2(0.01), padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', \n",
    "               kernel_regularizer=l2(0.01), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    audio_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = audio_model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, min_delta=0.01, mode='min', restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath='best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "    ])\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = audio_model.evaluate(x_test, y_test)\n",
    "    print(f\"Audio Model Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return audio_model, history\n",
    "\n",
    "# Train the audio model with pre-extracted features\n",
    "audio_model, history = build_model(x_train, y_train, x_test, y_test, num_classes=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming `Y` is the original list of labels (before one-hot encoding)\n",
    "# Fit the LabelEncoder on the original labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.argmax(y_train, axis=1))  # Fit LabelEncoder on the integer-encoded classes\n",
    "\n",
    "# Since `y_test` is one-hot encoded, convert it back to the original class labels\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = audio_model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions from one-hot encoding to class labels\n",
    "\n",
    "# Function to plot the confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Get the unique class labels using the fitted label encoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred_classes, class_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_features = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_features = X\n",
    "labels = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Emotions' column and pick the first sample for each emotion\n",
    "samples = data_path.groupby('Emotions').first().reset_index()\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['Path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "audio_file = samples['Path'][0]\n",
    "y, sr = librosa.load(audio_file, sr=16000)\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform of the Audio Sample')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plps = librosa.feature.plp(y=y, sr=sr, n_plp=13)\n",
    "plps = extract_plp_features(data, sr=sr)\n",
    "plt.figure(figsize=(12, 6))\n",
    "librosa.display.specshow(plps, sr=sr, x_axis='time', cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('plp Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('plp Coefficients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the audio file\n",
    "audio_file = samples['Path'][0]  # Replace with your audio file path\n",
    "y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "# Remove silences at the start and end\n",
    "y_trimmed, _ = librosa.effects.trim(y)\n",
    "\n",
    "# Calculate time array for waveform\n",
    "time = np.linspace(0, len(y_trimmed) / sr, len(y_trimmed))\n",
    "\n",
    "# Create Mel spectrogram\n",
    "S = librosa.feature.melspectrogram(y=y_trimmed, sr=sr, n_mels=128, fmax=8000)\n",
    "\n",
    "# Create PLP spectrogram\n",
    "PLPs = extract_plp_features(y_trimmed, sr)\n",
    "\n",
    "# Create spectrogram using STFT\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y_trimmed)), ref=np.max)\n",
    "\n",
    "# Create the figure with 4 subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(12, 16), sharex=True)\n",
    "\n",
    "# Plot the waveform\n",
    "axs[0].set_title('Waveform')\n",
    "axs[0].plot(time, y_trimmed, color='blue')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Plot the spectrogram\n",
    "axs[1].set_title('Spectrogram (STFT)')\n",
    "img1 = axs[1].imshow(\n",
    "    D, \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 0, sr // 2], \n",
    "    cmap='viridis'\n",
    ")\n",
    "axs[1].set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(img1, ax=axs[1], format=\"%+2.0f dB\")\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "axs[2].set_title('Mel Spectrogram')\n",
    "mel_time = np.linspace(0, len(y_trimmed) / sr, S.shape[1])  # Align Mel spectrogram timeline\n",
    "img2 = axs[2].imshow(\n",
    "    librosa.power_to_db(S, ref=np.max), \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 0, 8000], \n",
    "    cmap='magma'\n",
    ")\n",
    "axs[2].set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(img2, ax=axs[2], format=\"%+2.0f dB\")\n",
    "\n",
    "# Plot the PLP spectrogram\n",
    "axs[3].set_title('PLP Spectrogram')\n",
    "plp_time = np.linspace(0, len(y_trimmed) / sr, PLPs.shape[1])  # Align PLP spectrogram timeline\n",
    "img3 = axs[3].imshow(\n",
    "    PLPs, \n",
    "    aspect='auto', origin='lower', \n",
    "    extent=[0, len(y_trimmed) / sr, 1, PLPs.shape[0]], \n",
    "    cmap='cividis'\n",
    ")\n",
    "axs[3].set_ylabel('PLP Coefficients')\n",
    "fig.colorbar(img3, ax=axs[3], format=\"%+2.0f dB\")\n",
    "\n",
    "# Add a shared X-axis label\n",
    "plt.xlabel('Time (s)')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plps.shape\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming PLP features are in `plps` of shape (721, 13)\n",
    "# Create a DataFrame with PLP features\n",
    "plp_df = pd.DataFrame(plps, columns=[f'PLP {i+1}' for i in range(plps.shape[1])])\n",
    "\n",
    "# Plot the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=plp_df)\n",
    "plt.title('Boxplot of PLP Coefficients')\n",
    "plt.xlabel('PLP Coefficient Index')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plp_features.shape\n",
    "plp_features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.flatten()  # Ensures a 1D array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute mean across time steps for each sample\n",
    "mean_plp = np.mean(plp_features, axis=1)  # Shape: (n_samples, n_coefficients)\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "df = pd.DataFrame(mean_plp, columns=[f\"PLP {i+1}\" for i in range(mean_plp.shape[1])])\n",
    "df['Emotion'] = labels\n",
    "\n",
    "# Melt for grouped bar plot\n",
    "df_melted = df.melt(id_vars='Emotion', var_name='PLP Coefficient', value_name='Mean Value')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_melted, x='PLP Coefficient', y='Mean Value', hue='Emotion', ci=\"sd\")\n",
    "plt.title('Mean PLP Coefficients Across Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the desired plp coefficients\n",
    "selected_plp = [\"plp 1\", \"plp 2\", \"plp 4\", \"plp 6\"]\n",
    "df_selected = df.melt(id_vars='Emotion', var_name='plp Coefficient', value_name='Mean Value')\n",
    "df_selected = df_selected[df_selected['plp Coefficient'].isin(selected_plp)]\n",
    "\n",
    "# Plot the selected coefficients\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_selected, x='plp Coefficient', y='Mean Value', hue='Emotion', ci=\"sd\")\n",
    "plt.title('Mean of Selected plp Coefficients Across Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Emotion')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
